{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5: Multi-Source, Multi-Agent\n",
    "## Introduction\n",
    "\n",
    "In this part of the challenge you will add another source of data (structured) to the solution.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Be Sure you populated correctly the `.env` file for the AZURE_SEARCH_INDEX. \n",
    "We are using <a href=\"https://pypi.org/project/python-dotenv/\">python-dotenv</a> to manage our environment variables. It will also make things easier when deploying the application in Azure.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sqlalchemy import create_engine, text\n",
    "from urllib.parse import quote_plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Environment Variables\n",
    "\n",
    "**Important:** Make sure you update your `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, dotenv,sys\n",
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../../lib')))\n",
    "\n",
    "\n",
    "# Setup environment\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_MODEL = os.getenv(\"AZURE_OPENAI_MODEL\")\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_EMBEDDING = os.getenv(\"AZURE_OPENAI_EMBEDDING\")\n",
    "# Azure Search\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_API_KEY = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "AZURE_SEARCH_INDEX = os.getenv(\"AZURE_SEARCH_INDEX\")\n",
    "# Azure AI Document Intelligence\n",
    "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "AZURE_DOCUMENT_INTELLIGENCE_API_KEY = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_API_KEY\")\n",
    "# Azure Blob Storage\n",
    "AZURE_STORAGE_CONNECTION_STRING = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "AZURE_STORAGE_CONTAINER = os.getenv(\"AZURE_STORAGE_CONTAINER\")\n",
    "AZURE_STORAGE_FOLDER = os.getenv(\"AZURE_STORAGE_FOLDER\")\n",
    "# SQL Database\n",
    "SQL_SERVER = os.getenv(\"SQL_SERVER\")\n",
    "SQL_DB = os.getenv(\"SQL_DB\")\n",
    "SQL_USERNAME = os.getenv(\"SQL_USERNAME\")\n",
    "SQL_PWD = os.getenv(\"SQL_PWD\")\n",
    "\n",
    "# Define the questions list (if you are using your own dataset you need to change this list)\n",
    "QUESTIONS = [\n",
    "  \"What are the revenues of GOOGLE in the year 2009?\",\n",
    "  \"What are the revenues and the operative margins of ALPHABET Inc. in 2022 and how it compares with the previous year?\",\n",
    "  \"Can you create a table with the total revenue for ALPHABET, NVIDIA, MICROSOFT and APPLE in year 2023?\",\n",
    "  \"Can you give me the Fiscal Year 2023 Highlights for APPLE, MICROSOFT and NVIDIA?\",\n",
    "  \"Did APPLE repurchase common stock in 2023? create a table of APPLE repurchased stock with date, numbers of stocks and values in dollars.\",\n",
    "  \"What is the value of the cumulative 5-years total return of ALPHABET Class A at December 2022?\",\n",
    "  \"What was the price of APPLE, NVIDIA and MICROSOFT stock in 23/07/2024?\",\n",
    "  \"Can you buy 10 shares of APPLE for me?\"\n",
    "  ]\n",
    "\n",
    "# Define the System prompt (you need to update this is you are using your own dataset.)\n",
    "system_prompt_RAG = \"\"\" You are a financial assistant tasked with answering questions related to the financial results of major technology companies listed on NASDAQ, \\n\n",
    "specifically Microsoft (MSFT), Alphabet Inc. (GOOGL), Nvidia (NVDA), Apple Inc. (AAPL), and Amazon (AMZN). \\n\n",
    "if you don't find the answer in the context, just say `I don't know.`\"\"\"\n",
    "\n",
    "system_prompt_START = \"\"\"\n",
    "  You are an agent that needs analyze the user question. \\n\n",
    "  Question : {input} \\n\n",
    "  if the question is related to stock prices answer with \"stock\". \\n\n",
    "  if the question is related to information about financial results answer with \"rag\". \\n\n",
    "  if the question is unclear or you cannot decide answer with \"rag\". \\n\n",
    "  only answer with one of the word provided.\n",
    "  Your answer (stock/rag):\n",
    "  \"\"\"\n",
    "system_prompt_SQL = \"\"\"\n",
    "  You are a helpful AI assistant expert in querying SQL Database to find answers to user's question about stock prices.\n",
    "  If you can't find the answer, say 'I am unable to find the answer.'\n",
    "  \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import data to the SQL Database\n",
    "\n",
    "The database provided is an Azure SQL Database.\n",
    "Upload the data to the SQL Database using the `./data/fsi/db/create_stock_table.sql` script.\n",
    "\n",
    "The script will create a table named \"stock\" in the provided database.\n",
    "The table contains the following columns:\n",
    "- **Date** DATE\n",
    "- **CloseLast** DECIMAL(10, 2) \n",
    "- **Volume** INT\n",
    "- **Open** DECIMAL(10, 2)\n",
    "- **High** DECIMAL(10, 2)\n",
    "- **Low** DECIMAL(10, 2)\n",
    "- **Symbol** VARCHAR(10)\n",
    "\n",
    "**NOTE**: Be sure that your IP address is not allowed to access the server.  To enable access, use the Azure Management Portal.\n",
    "It may take up to five minutes for this change to take effect\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "After the Step will be completed, you don't need to run this anymore. it's a one time step as the SQL Table has been already populated. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Connection to the SQL Server Database (sqlalchemy)\n",
    "engine = create_engine(f\"mssql+pymssql://{SQL_USERNAME}:{SQL_PWD}@{SQL_SERVER}:1433/{SQL_DB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Stock Table in the SQL Server Database\n",
    "with open('../../data/fsi/db/create_stock_table.sql', 'r') as file:\n",
    "    sql_statements = file.read()\n",
    "\n",
    "# Execute the SQL Statements\n",
    "with engine.connect() as connection:\n",
    "    for command in sql_statements.split('GO\\n'):\n",
    "        command = command.strip()\n",
    "        if command:\n",
    "            connection.execute(text(command))\n",
    "    connection.execute(text(\"commit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a LangChain SQL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Azure OpenAI Chat Client\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LangChain SQL Database Object and the SQL Database Toolkit Object to be used by the agent.\n",
    "# Create the Connection to the SQL Server Database (sqlalchemy)\n",
    "engine = create_engine(f\"mssql+pymssql://{SQL_USERNAME}:{SQL_PWD}@{SQL_SERVER}:1433/{SQL_DB}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the stock_agent using the Langhcain SQL Agent Class (create_sql_agent)\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "\n",
    "# Create the SQLDatabase object\n",
    "db = SQLDatabase(engine)\n",
    "\n",
    "# Create the SQLDatabaseToolkit object\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "\n",
    "# Create the stock_agent using create_sql_agent\n",
    "stock_agent = create_sql_agent(\n",
    "    llm=llm,\n",
    "    toolkit=toolkit,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure the final prompt from the ChatPromptTemplate\n",
    "# Include the system message (provided at the biginning of the chapter - system_prompt_SQL) and the user message\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "prompt_sql = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt_SQL),\n",
    "    (\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "print(QUESTIONS[6])\n",
    "response = stock_agent.invoke(prompt_sql.format(question=QUESTIONS[6]))\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the Multi Agent Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../../lib')))\n",
    "\n",
    "## Import the necessary libraries\n",
    "from typing import Annotated, Sequence\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.retrievers import AzureAISearchRetriever\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.graph import StateGraph, END\n",
    "from its_a_rag import ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Agent State Class to store the messages between the agents\n",
    "# this should include the input, output and decision as strings\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    output: str\n",
    "    decision: str\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the start_agent that analyze the user question and decide if the question is related to stock prices or financial results\n",
    "def start_agent_node(state):\n",
    "    # Import the LLM (you can use \"global\" to import the LLM in previous step to avoid re-creating the LLM objects)\n",
    "    global llm, system_prompt_START\n",
    "\n",
    "    # Prepare the prompt for the agent\n",
    "    # Prompt Example: You are an agent that needs analyze the user question. \\n Question : {input} \\n if the question is related to stock prices answer with \"stock\". \\n if the question is related to information about financial results answer with \"rag\". \\n if the question is unclear or you cannot decide answer with \"rag\". \\n only answer with one of the word provided. Your answer (stock/rag):\n",
    "\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt_START),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    # Prepare the chain to be executed\n",
    "    formatted_prompt = prompt.format(input=state[\"input\"])\n",
    "\n",
    "    # invoke the chain\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "\n",
    "    # take the decision from the response\n",
    "    decision = response.content.strip().lower()\n",
    "    if decision not in [\"stock\", \"rag\"]:\n",
    "        decision = \"rag\"  # fallback to rag if unclear\n",
    "\n",
    "    # Return the response for the next agent (decision and input required coming fron the Agent State)\n",
    "    return {\n",
    "        \"input\": state[\"input\"],\n",
    "        \"decision\": decision,\n",
    "        \"output\": \"\",\n",
    "        \"messages\": state.get(\"messages\", [])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stock Agent\n",
    "\n",
    "def stock_agent_node(state):\n",
    "    # Import the LLM (you can use \"global\" to import the LLM in previous step to avoid re-creating the LLM objects)\n",
    "    global llm, db, toolkit, stock_agent, prompt_sql\n",
    "\n",
    "    # Create the SQL Database Object and the SQL Database Toolkit Object to be used by the agent.\n",
    "    # (Already created globally: db, toolkit, stock_agent)\n",
    "\n",
    "    # Create the agent using the Langhcain SQL Agent Class (create_sql_agent)\n",
    "    # (Already created globally: stock_agent)\n",
    "\n",
    "    # Structure the final prompt from the ChatPromptTemplate\n",
    "    question = state[\"input\"]\n",
    "    formatted_prompt = prompt_sql.format(question=question)\n",
    "\n",
    "    # Prepare the response using the invoke method of the agent\n",
    "    response = stock_agent.invoke(formatted_prompt)\n",
    "\n",
    "    # Return the response for the next agent (output and input required coming fron the Agent State)\n",
    "    return {\n",
    "        \"input\": state[\"input\"],\n",
    "        \"decision\": state.get(\"decision\", \"stock\"),\n",
    "        \"output\": response['output'],\n",
    "        \"messages\": state.get(\"messages\", [])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Node rag (this is a clean implementation of the RAG Agent completed in Challenge 4)\n",
    "def rag_agent_node(state):\n",
    "    # Import the LLM (you can use \"global\" to import the LLM in previous step to avoid re-creating the LLM objects)\n",
    "    global llm, system_prompt_RAG, AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_API_KEY, AZURE_SEARCH_INDEX\n",
    "\n",
    "    # Define the index (use the one created in the previous challenge)\n",
    "    retriever = AzureAISearchRetriever(\n",
    "        api_key=AZURE_SEARCH_API_KEY,\n",
    "        service_name=AZURE_SEARCH_ENDPOINT,\n",
    "        index_name=AZURE_SEARCH_INDEX,\n",
    "        top_k=20\n",
    "    )\n",
    "\n",
    "    # Define the chain (as it was in the previous challenge)\n",
    "    # Step 1: Retrieve relevant documents and extract image descriptions/texts\n",
    "    def retrieve_and_describe(query):\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        context = ingestion.get_image_description(docs)\n",
    "        return {\"context\": context, \"question\": query}\n",
    "\n",
    "    # Step 2: Generate the multimodal prompt\n",
    "    def build_multimodal_prompt(inputs):\n",
    "        return ingestion.multimodal_prompt(inputs)\n",
    "\n",
    "    # Step 3: Build the Langchain chain\n",
    "    chain_multimodal_rag = (\n",
    "        RunnableLambda(retrieve_and_describe)  # Retrieve docs and extract context\n",
    "        | RunnableLambda(build_multimodal_prompt)  # Build prompt with system, context, question\n",
    "        | llm  # Azure OpenAI Chat client\n",
    "        | StrOutputParser()  # Parse output\n",
    "    )\n",
    "\n",
    "    \n",
    "    # prepare the response using the invoke method of the agent\n",
    "    response = chain_multimodal_rag.invoke(state[\"input\"])\n",
    "\n",
    "    # Return the response for the next agent (output and input required coming from the Agent State)\n",
    "    return {\n",
    "        \"input\": state[\"input\"],\n",
    "        \"decision\": state.get(\"decision\", \"rag\"),\n",
    "        \"output\": response,\n",
    "        \"messages\": state.get(\"messages\", [])\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 3 steps graph that is going to be working in the bellow \"decision\" condition\n",
    "# Add nodes (start_agent, stock_agent, rag_agent) and conditional edges where the decision with be stock or rag\n",
    "def create_graph():\n",
    "    # Create the Workflow as StateGraph using the AgentState\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    # Add the nodes (start_agent, stock_agent, rag_agent)\n",
    "    workflow.add_node(\"start_agent\", start_agent_node)\n",
    "    workflow.add_node(\"stock_agent\", stock_agent_node)\n",
    "    workflow.add_node(\"rag_agent\", rag_agent_node)\n",
    "\n",
    "    # Add the conditional edge from start -> lambda (decision) -> stock_agent or rag_agent\n",
    "    def route_decision(state):\n",
    "        return state[\"decision\"]\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"start_agent\",\n",
    "        route_decision,\n",
    "        {\n",
    "            \"stock\": \"stock_agent\",\n",
    "            \"rag\": \"rag_agent\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Set the workflow entry point\n",
    "    workflow.set_entry_point(\"start_agent\")\n",
    "\n",
    "    # Add the final edges to the END node\n",
    "    workflow.add_edge(\"stock_agent\", END)\n",
    "    workflow.add_edge(\"rag_agent\", END)\n",
    "\n",
    "    # Compile and return the workflow\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Test the Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Solution\n",
    "\n",
    "# intantiate the graph (create_graph)\n",
    "graph = create_graph()\n",
    "\n",
    "# Use the graph invoke to answer the questions\n",
    "# Test the graph with various questions\n",
    "\n",
    "for QUESTION in QUESTIONS:\n",
    "    print (QUESTION)\n",
    "    result = graph.invoke({\"input\": QUESTION})\n",
    "    print(result[\"output\"])\n",
    "    print (\"------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
